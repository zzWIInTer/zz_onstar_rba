{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qqlVIlqywJT"
   },
   "source": [
    "# Data Pre-processing & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZM9-fB7nMvK"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#\n",
    "#  Copyright 2021 Google Inc.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "# This solution, including any related sample code or data, is made available\n",
    "# on an “as is,” “as available,” and “with all faults” basis, solely for\n",
    "# illustrative purposes, and without warranty or representation of any kind.\n",
    "# This solution is experimental, unsupported and provided solely for your\n",
    "# convenience. Your use of it is subject to your agreements with Google, as\n",
    "# applicable, and may constitute a beta feature as defined under those\n",
    "# agreements.  To the extent that you make any data available to Google in\n",
    "# connection with your use of the solution, you represent and warrant that you\n",
    "# have all necessary and appropriate rights, consents and permissions to permit\n",
    "# Google to use and process that data.  By using any portion of this solution,\n",
    "# you acknowledge, assume and accept all risks, known and unknown, associated\n",
    "# with its usage, including with respect to your deployment of any portion of\n",
    "# this solution in your systems, or usage in connection with your business,\n",
    "# if at all.\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh6JyZGLcEgE"
   },
   "source": [
    "## 0) Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajeI2JqEf6Vd"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "######################### CHANGE BQ PROJECT NAME BELOW #########################\n",
    "################################################################################\n",
    "\n",
    "project_name = 'OnStar_RBA' #add proj name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aupXJjFcHkq"
   },
   "outputs": [],
   "source": [
    "# Google credentials authentication libraries\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# data processing libraries\n",
    "import numpy as np\n",
    "from numpy.core.numeric import NaN\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install boruta #boruta for feature selection\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# modeling and metrics\n",
    "from scipy.optimize import least_squares\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "import itertools\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 5] #change size of plot\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# BigQuery Magics\n",
    "'''\n",
    "BigQuery magics are used to run BigQuery SQL queries in a python environment.\n",
    "These queries can also be run in the BigQuery UI\n",
    "'''\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import magics\n",
    "\n",
    "magics.context.project = project_name\n",
    "\n",
    "client = bigquery.Client(project=magics.context.project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4W_dqFdtODF"
   },
   "source": [
    "## 1) Import dataset\n",
    "#####Only one of the following sections should be executed, either import from CSV, BigQuery, or Google Sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_O9o8SQ1LJB"
   },
   "source": [
    "##Import Original Data from BigQuery (Option #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzNoSH481KXT"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "auth.authenticate_user()\n",
    "bigquery.USE_LEGACY_SQL = False\n",
    "\n",
    "magics.context.project = project_name\n",
    "client = bigquery.Client(project=magics.context.project)\n",
    "%load_ext google.cloud.bigquery\n",
    "project_id = project_name\n",
    "dataset_name = '' #@param {type:\"string\"}\n",
    "table_name = '' #@param {type:\"string\"}\n",
    "query = 'select * from `' +  dataset_name + '.'+ table_name + '`'\n",
    "df = pd.io.gbq.read_gbq(query, project_id=project_id, dialect='standard')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-_u03bR1ffg"
   },
   "source": [
    "## Upload Original Data CSV (Option #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw2WhqiS1lBc"
   },
   "outputs": [],
   "source": [
    "#update file name\n",
    "file_name = ''\n",
    "df = pd.read_csv(file_name)\n",
    "df.fillna(0, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_RNvdpa1t4m"
   },
   "source": [
    "## Import Data from Sheets (Option #3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biIzkDdB11Kf"
   },
   "outputs": [],
   "source": [
    "import gspread\n",
    "from google.auth import default\n",
    "creds, _ = default()\n",
    "gc = gspread.authorize(creds)\n",
    "auth.authenticate_user()\n",
    "sheet_name = '' #@param {type:\"string\"}\n",
    "worksheet = gc.open(sheet_name).sheet1\n",
    "\n",
    "# get_all_values gives a list of rows.\n",
    "rows = worksheet.get_all_values()\n",
    "\n",
    "# Convert to a DataFrame and render.\n",
    "original_data_import = pd.DataFrame.from_records(rows)\n",
    "df = original_data_import.rename(\n",
    "    columns=original_data_import.iloc[0]).drop(\n",
    "        original_data_import.index[0]).reset_index(drop=True).astype('float')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU4CTzXPET0S"
   },
   "source": [
    "The first step is to remove variables that won't be used in the model. In this example, we remove columns like geo which is consistent across the dataset and aggregated media such as total clicks across DSPs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKXZi_hlDrj8"
   },
   "outputs": [],
   "source": [
    "df.drop(columns = ['geo','x1','x2','x8','x18','x19','x20','x21','x22','x23','x24','x25'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Fj7Ja1d9HmE"
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhiairgY8-XL"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMUNuZnoDrto"
   },
   "outputs": [],
   "source": [
    "#Set the date as index\n",
    "date_col = \"date\" #@param {type:\"string\"}\n",
    "df = df.sort_values(date_col).set_index(date_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vrz4EdICwoj"
   },
   "outputs": [],
   "source": [
    "'''If using daily data in the final model, set to True.\n",
    "If data is already at the weekly level OR if you'd like to\n",
    "roll daily data up to weekly level, leave as false\n",
    "'''\n",
    "is_daily_data = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qnMk7hcCh3s"
   },
   "source": [
    "Option to aggregate daily data to weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7_-23QJClBS"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run code below if you'd like to roll daily data up to weekly level.\n",
    "'''\n",
    "if is_daily_data == False:\n",
    "  df = df.resample('7D').sum()\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bum7dqyoC8Oa"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBmArInZ00VB"
   },
   "source": [
    "## 2) Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Bfetmm51Kcv"
   },
   "source": [
    "### 2.1) Check for missing data and impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCj0Ek9uEcIh"
   },
   "source": [
    "Check the amount of of missing values (% of total column) in the data and sort by\n",
    "highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuVFwXoa1AHY"
   },
   "outputs": [],
   "source": [
    "missing_values = 100*df.isnull().sum()/len(df)\n",
    "missing_values.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udPqb6_cEhTm"
   },
   "source": [
    "If there are any NAs in the data that should be zeros, replace those data\n",
    "points with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYqfqsqetb5_"
   },
   "outputs": [],
   "source": [
    "df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtcr2IYbmG4E"
   },
   "source": [
    "## 3) Define Y (KPI column) and create initial feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjJzWX6ulXYt"
   },
   "outputs": [],
   "source": [
    "#Input column names for Y (ex: \"new_accounts\" or \"sales\")\n",
    "kpi_col = \"y1\" #@param {type:\"string\"}\n",
    "target_variable = df[kpi_col] #y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ektWbmu5HPVk"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for features (all variables except date and kpi) x variables\n",
    "featureset_df = df[df.columns[df.columns != date_col]]\n",
    "featureset_df = df[df.columns[df.columns != kpi_col]]\n",
    "# Converting data to float\n",
    "featureset_df = featureset_df.astype(float, copy=False)\n",
    "featureset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgwVdkgCYa59"
   },
   "source": [
    "## 4) Visualize Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbMjymbzFH5W"
   },
   "source": [
    "Optional:\n",
    "\n",
    "Visualizing each series is useful to better understand the underlying distribution of the data. This allows for examination of outliers.\n",
    "\n",
    "Understanding the distribution of the underlying data can also inform prior parameterization in bayesian modeling approaches later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNg6FK5vYeU_"
   },
   "outputs": [],
   "source": [
    "for i in range(2,len(featureset_df.columns)):\n",
    "  plt.figure()\n",
    "  sns.kdeplot(featureset_df[featureset_df.columns[i]], label = featureset_df.\n",
    "              columns[i], fill = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6guckVPESzS5"
   },
   "source": [
    "## 5) Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf-gq2HQT6hl"
   },
   "source": [
    "### 5.1) Check for Seasonality and add Flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0YLKiyNFbBz"
   },
   "source": [
    "View the target variable as a time series plot and identify periods where data peaks.\n",
    "\n",
    "We also add flags for periods of peak seasonality such as Q2, Q3, and major winter holidays.\n",
    "\n",
    "Note: Please update seasonality with regards to your specific data requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B171c0iiT-Gz"
   },
   "outputs": [],
   "source": [
    "fig = px.line(df[kpi_col])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x487wMOUY6T"
   },
   "outputs": [],
   "source": [
    "featureset_df['Is_Q2Q3'] = (df.index.get_level_values(0).month == 4).astype(int) |\n",
    "  (df.index.get_level_values(0).month == 5).astype(int) |\n",
    "  (df.index.get_level_values(0).month == 6).astype(int) |\n",
    "  (df.index.get_level_values(0).month == 7).astype(int) |\n",
    "  (df.index.get_level_values(0).month == 8).astype(int) |\n",
    "  (df.index.get_level_values(0).month == 9).astype(int)\n",
    "\n",
    "featureset_df['Is_Holiday'] = ((df.index == '2017-11-17') |\n",
    "                               (df.index == '2017-12-22') |\n",
    "                               (df.index == '2018-11-16') |\n",
    "                               (df.index == '2018-12-21') |\n",
    "                               (df.index == '') | (df.index == ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3AGZk-cT-zj"
   },
   "source": [
    "### 5.2) Applying Adstock Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8FKlBYJFk7V"
   },
   "source": [
    "We'll need to transform the raw data by applying Adstock to have it most accurately predict the target variable.\n",
    "\n",
    "**Adstock:** Applies an infinite lag that decreases its weight as time passes.\n",
    "Media effect on sales may lag behind the original exposure and extend several weeks. More info can be found here: https://en.wikipedia.org/wiki/Advertising_adstock\n",
    "\n",
    "\n",
    "Adstock takes 3 parameters (L, P, D):\n",
    "\n",
    "L: length of the media effect\n",
    "\n",
    "P: peak/delay of the media effect, how many days/weeks it’s lagging behind first exposure\n",
    "\n",
    "D: decay/retention rate of the media channel, concentration of the effect\n",
    "\n",
    "**First, split the df into two different dataframes:**\n",
    "\n",
    "1. Features that **don't** need to be transformed\n",
    "      - Examples are:\n",
    "          - date\n",
    "          - target variable\n",
    "          - control variables (seasonality, promotions, etc.)\n",
    "\n",
    "2. Features that **do** need to be transformed\n",
    "      -  Paid media tactics\n",
    "      -  Any other feature where there is some sort of delayed response with the target variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXZAGMDmX-Vr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Variables that do not need to be transformed (i.e. Target variable + controls)\n",
    "\n",
    "Note: In this example case, almost all of the features in the featureset_df\n",
    "are media features.As more dummy variables or other control variables are added,\n",
    "the user will need to specify which columns should be transformed\n",
    "'''\n",
    "\n",
    "untransformed_df = pd.concat([target_variable,\n",
    "                              featureset_df[['Is_Q2Q3','Is_Holiday']]],\n",
    "                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwllbKVnYXwM"
   },
   "outputs": [],
   "source": [
    "transformed_df_partial = featureset_df.loc[:,~featureset_df.columns.isin(\n",
    "    untransformed_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfQfeIlOMRvj"
   },
   "source": [
    "#### 5.2.1) Create the transformation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzf0VFGYmDYj"
   },
   "source": [
    "This below function creates all permutations of the Adstock parameters and then calculates the correlation between each variable and the Y variable. Returns the top 3 highest correlated features.\n",
    "\n",
    "L: fixed value. 8 if data is weekly and 21 if data is daily\n",
    "\n",
    "P: multiple values between 1 - 7\n",
    "\n",
    "D: multiple values between 0 - 0.48\n",
    "\n",
    "Please note, the parameters can be adjusted in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UyZzxVBxNGk"
   },
   "outputs": [],
   "source": [
    "def createTransformations(df1, df2):\n",
    "  columns = df2.columns\n",
    "  sales = df1[[kpi_col]]\n",
    "  all_data = []\n",
    "  for col in columns:\n",
    "    newdfcolumns = []\n",
    "    for l in np.linspace(21 if is_daily_data else 8,\n",
    "                         21 if is_daily_data else 8, 1):\n",
    "      for p in np.round(np.linspace(0, 7, 7),2):\n",
    "        for d in np.round(np.linspace(0, 0.48, 9),2):\n",
    "          newdf = pd.DataFrame()\n",
    "          newdf[f'{col}l{l}p{p}d{d}'.replace('.','_')] = Transformation(\n",
    "              df2[col], col, int(2), p, d)\n",
    "          newdfcolumns.append(newdf)\n",
    "    concat_new_df = pd.concat(newdfcolumns, axis=1)\n",
    "    corr_df = pd.concat([sales, concat_new_df], axis=1)\n",
    "    corr = abs(corr_df.corr()).sort_values(kpi_col, ascending=False)\n",
    "    new_vals= corr.iloc[1:4 , 0:1].index.tolist()\n",
    "    data = concat_new_df[new_vals]\n",
    "    all_data.append(data)\n",
    "  final_data = pd.concat(all_data,axis=1)\n",
    "\n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuhDt4spxb0c"
   },
   "outputs": [],
   "source": [
    "#This function creates every combination of Adstock\n",
    "\n",
    "def Transformation(data, x, L, P, D):\n",
    "    x_orig = data\n",
    "    x_0 = np.append(np.zeros(L-1), x_orig)\n",
    "    weights = np.zeros(L)\n",
    "    for l in range(L):\n",
    "      weight = D**((l-P)**2.0)\n",
    "      weights[L-1-l] = weight\n",
    "    adstocked_x = []\n",
    "    for i in range(L-1, len(x_0)):\n",
    "      x_array = x_0[i-L+1:i+1]\n",
    "      xi = sum(x_array * weights)/sum(weights)\n",
    "      adstocked_x.append(xi)\n",
    "    return adstocked_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVKeMikfMWG9"
   },
   "source": [
    "#### 5.2.1) Execute the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5G4HnsCxcco"
   },
   "outputs": [],
   "source": [
    "transformed_df_partial = transformed_df_partial.sort_values(date_col)\n",
    "sys.setrecursionlimit(len(transformed_df_partial.index)+100)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "  warnings.simplefilter('ignore')\n",
    "  transformed_df_partial = createTransformations(untransformed_df,\n",
    "                                                 transformed_df_partial)\n",
    "transformed_df_partial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rEpk2EWYuRW"
   },
   "source": [
    "### 6) Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ojhRBywZJCY"
   },
   "source": [
    "6.1) Train/Test Split & Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITDnZ121uB_O"
   },
   "source": [
    "Please specify the test dataset size below.\n",
    "\n",
    "Completing train/test split prior to standardizing and feature selection ensures no data leakate.\n",
    "\n",
    "**Note:** use the same test % size when building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaVe09gIuAG8"
   },
   "outputs": [],
   "source": [
    "test_size_percentage = 0.1 #@param {type:\"slider\", min:0, max:0.3, step:0.01}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYhi4IjlH9Iz"
   },
   "source": [
    "The default method of standardization utilizes Standard Scaler, which takes in\n",
    "input data and transforms so that the output has mean 0 and standard deviation of 1\n",
    "across all features.\n",
    "\n",
    "\n",
    "Alternative methods of feature scaling include square-root transformation,\n",
    "de-meaning, natural log transformations, Min-Max Scalers, or normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsh3UTusZZeR"
   },
   "outputs": [],
   "source": [
    "test_size = round(len(transformed_df_partial.index)*test_size_percentage)\n",
    "\n",
    "untransformed_df.reset_index(inplace=True, drop=True)\n",
    "transformed_df = pd.concat(\n",
    "    [transformed_df_partial, untransformed_df[untransformed_df.columns[\n",
    "        untransformed_df.columns != kpi_col]]], axis = 1)\n",
    "dataset_split = train_test_split(transformed_df,\n",
    "                                 untransformed_df[[kpi_col]],\n",
    "                                 test_size=test_size, shuffle=False)\n",
    "\n",
    "transformed_df_train = dataset_split[0]\n",
    "transformed_df_test = dataset_split[1]\n",
    "transformed_df_train_target = dataset_split[2]\n",
    "transformed_df_test_target = dataset_split[3]\n",
    "train_scaler = StandardScaler()\n",
    "standardized_array_train = train_scaler.fit_transform(transformed_df_train)\n",
    "standardized_array_test = train_scaler.transform(transformed_df_test)\n",
    "standardized_df_test = pd.DataFrame(\n",
    "    standardized_array_test, columns = transformed_df_test.columns)\n",
    "standardized_df_train = pd.DataFrame(\n",
    "    standardized_array_train, columns = transformed_df_train.columns)\n",
    "standardized_df_test.reset_index(inplace=True, drop=True)\n",
    "standardized_df_train.reset_index(inplace=True, drop=True)\n",
    "transformed_df_test_target.reset_index(inplace=True, drop=True)\n",
    "transformed_df_train_target.reset_index(inplace=True, drop=True)\n",
    "\n",
    "trimmed_transformed_standardized_train_w_target = pd.concat(\n",
    "    [transformed_df_train_target, standardized_df_train], axis=1)\n",
    "trimmed_transformed_standardized_train_w_o_target = standardized_df_train\n",
    "trimmed_transformed_standardized_test_w_target = pd.concat(\n",
    "    [transformed_df_test_target, standardized_df_test], axis=1)\n",
    "trimmed_transformed_standardized_test_w_o_target = standardized_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd4Jx-VgIFV4"
   },
   "source": [
    "Option to review visuals of the data. After the data is standardized the distributions may take on a more normal shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IapNWY0XZc9r"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "for i in range(0,len(trimmed_transformed_standardized_train_w_o_target.columns)):\n",
    "  plt.figure()\n",
    "  sns.kdeplot(trimmed_transformed_standardized_train_w_o_target[\n",
    "      trimmed_transformed_standardized_train_w_o_target.columns[i]], label =\n",
    "      trimmed_transformed_standardized_train_w_o_target.columns[i], shade = True)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3DlsBG_Izo_"
   },
   "source": [
    "## 7) Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6VoaFdyHRsV"
   },
   "source": [
    "For feature selection we employ the Boruta algorithm.[(More information here)](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a\n",
    ")\n",
    "\n",
    "This algorithm will tell you the rank of each feature and whether or not to keep a varaible in the model (i.e. Keep  = True/False). The goal of RBA is to optimize across all paid digital media tactics, therefore select the top ranking feature for each group of features (whether or not the algorithm tells you to keep the feature).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXWDCqcVF0KX"
   },
   "outputs": [],
   "source": [
    "# Specifiying the target and x variables\n",
    "num_train_samples = transformed_df_train_target.shape[0]\n",
    "\n",
    "y = transformed_df_train_target.to_numpy().reshape(num_train_samples,)\n",
    "x = trimmed_transformed_standardized_train_w_o_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fk4eshicHtQZ"
   },
   "outputs": [],
   "source": [
    "# define random forest classifier\n",
    "forest = RandomForestRegressor(n_jobs=-1, max_depth=5)\n",
    "forest.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B8sMMKWHwOm"
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "  warnings.simplefilter('ignore')\n",
    "  # define Boruta feature selection method\n",
    "  feat_selector = BorutaPy(\n",
    "      forest, n_estimators='auto', verbose=0, random_state=1)\n",
    "\n",
    "  # find all relevant features\n",
    "  feat_selector.fit(np.array(x), np.array(y))\n",
    "\n",
    "  # check selected features\n",
    "  feat_selector.support_\n",
    "\n",
    "  # check ranking of features\n",
    "  feat_selector.ranking_\n",
    "\n",
    "  # call transform() on X to filter it down to selected features\n",
    "  X_filtered = feat_selector.transform(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kax9LfLYH44N"
   },
   "outputs": [],
   "source": [
    "#Select the top ranking variable for each group of variables.\n",
    "feature_ranks = list(zip(x.columns,\n",
    "                         feat_selector.ranking_,\n",
    "                         feat_selector.support_))\n",
    "\n",
    "# iterate through and print out the results\n",
    "for feat in feature_ranks:\n",
    "    print('{:<25}, Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xM0Uhm3HxaL"
   },
   "source": [
    "Function to reduce the overall dataset to the lowest rank feature from the Boruta output, and save to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGO_akgFzTUb"
   },
   "outputs": [],
   "source": [
    "def stripQualifiers(column, leading=True):\n",
    "  retString = column\n",
    "  if retString.startswith('cost_') and leading:\n",
    "    retString = retString.split('cost_')[1]\n",
    "  if retString.startswith('Paid_') and leading:\n",
    "    retString = retString.split('Paid_')[1]\n",
    "  try:\n",
    "    int(retString[::-1][retString[::-1].find('l')-1])\n",
    "    retString = retString[::-1][retString[::-1].find('l')+1:][::-1]\n",
    "    return retString\n",
    "  except:\n",
    "    return retString\n",
    "\n",
    "keepers = {}\n",
    "for feat in feature_ranks:\n",
    "    #print('{:<25}, Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "    featureName = stripQualifiers(feat[0], True)\n",
    "    if keepers.get(featureName) is None:\n",
    "      keepers[featureName] = {'fullColumnName': feat[0], 'rank': feat[1]}\n",
    "    else:\n",
    "      if keepers[featureName]['rank'] > feat[1]:\n",
    "        keepers[featureName] = {'fullColumnName': feat[0], 'rank': feat[1]}\n",
    "\n",
    "selected_featureset_df = trimmed_transformed_standardized_train_w_o_target.drop(\n",
    "    columns=[col for col in trimmed_transformed_standardized_train_w_o_target\n",
    "             if col not in [keepers[i]['fullColumnName'] for i in keepers.keys()]])\n",
    "\n",
    "selected_featureset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE0Prw-gmXBs"
   },
   "source": [
    "## 8) Handle Multicollinearity (reduce feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDrFJIAxIJ2h"
   },
   "source": [
    "1. Print a correlation heatmap to visualize correlations across feature set\n",
    "2. Run variance inflation factor analysis and output results to flag multicollinearity above specified threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8onrNK_JZsZ4"
   },
   "outputs": [],
   "source": [
    "correl = selected_featureset_df.corr()\n",
    "\n",
    "# Getting the Upper Triangle of the co-relation matrix\n",
    "matrix = np.triu(correl)\n",
    "\n",
    "# using the upper triangle matrix as mask\n",
    "sns.heatmap(correl, mask=matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QQTR1g1N839"
   },
   "source": [
    "Run VIF analysis and flag values greater than 10.\n",
    "\n",
    "Industry best practice flags values above 10 as an extreme violation of regression model assumptions. [(Reference)](https://en.wikipedia.org/wiki/Variance_inflation_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD6FJ6SDn-ic"
   },
   "outputs": [],
   "source": [
    "vif = add_constant(selected_featureset_df)\n",
    "\n",
    "# loop to calculate the VIF for each X\n",
    "vif = pd.Series([variance_inflation_factor(vif.values, i)\n",
    "      for i in range(vif.shape[1])],\n",
    "      index=vif.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbr437wVwIoc"
   },
   "outputs": [],
   "source": [
    "# processing to output VIF results as a dataframe\n",
    "vif_df=vif.to_frame().reset_index()\n",
    "\n",
    "vif_df.columns = ['feature', 'vif']\n",
    "vif_df=vif_df.replace([np.inf], np.nan)\n",
    "vif_df=vif_df.fillna(0).sort_values(by=\"vif\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6fSf3Hyw1gw"
   },
   "outputs": [],
   "source": [
    "vif_df.reset_index(inplace = True)\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NLieHb1OH3q"
   },
   "source": [
    "Drop the highest VIF features and print the high collinearity columns in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhmaytzMZJy7"
   },
   "outputs": [],
   "source": [
    "high_collinearity_columns = vif_df.feature[vif_df['vif'] >= 10].to_list()\n",
    "high_collinearity_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwJ2RObPOL0Y"
   },
   "source": [
    "Drop 1 variable at a time (start with the highest VIF) and re-run the VIF cell to re-check multicollinearity. This will allow the user to preserve as many features in the model as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pJIafpZXs5B"
   },
   "outputs": [],
   "source": [
    "cols_to_drop = []\n",
    "while vif_df.vif[1] >= 10:\n",
    "  if vif_df.vif[1] >= 10:\n",
    "    cols_to_drop.append(vif_df.feature[1])\n",
    "    selected_featureset_df.drop(columns = vif_df.feature[1],inplace = True)\n",
    "    vif = add_constant(selected_featureset_df)\n",
    "  # loop to calculate the VIF for each X\n",
    "    vif = pd.Series([variance_inflation_factor(vif.values, i)\n",
    "    for i in range(vif.shape[1])], index=vif.columns)\n",
    "    # processing to output VIF results as a dataframe\n",
    "    vif_df=vif.to_frame().reset_index()\n",
    "    vif_df.columns = ['feature', 'vif']\n",
    "    vif_df=vif_df.replace([np.inf], np.nan)\n",
    "    vif_df=vif_df.fillna(0).sort_values(by=\"vif\", ascending=False)\n",
    "    vif_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFEa0NTUY9W_"
   },
   "outputs": [],
   "source": [
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmOql8Li9IeX"
   },
   "outputs": [],
   "source": [
    "selected_featureset_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3hI1pqZM02U"
   },
   "outputs": [],
   "source": [
    "len(selected_featureset_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc7SmJGmDjPr"
   },
   "source": [
    "## 9) Export Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qVnQK-lbOzl"
   },
   "source": [
    "### 9.1) Export data for model building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwvhPcKQwGwd"
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([trimmed_transformed_standardized_train_w_o_target,\n",
    "                      trimmed_transformed_standardized_test_w_o_target])\n",
    "final_df = final_df[final_df.columns[final_df.columns.isin(\n",
    "    selected_featureset_df.columns)]]\n",
    "final_df.reset_index(inplace=True, drop=True)\n",
    "final_df[kpi_col] = target_variable.reset_index()[kpi_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOlFNnE7EHpR"
   },
   "outputs": [],
   "source": [
    "final_df[date_col] = df.index #add back in the date as a separate column from the index\n",
    "final_df[date_col] = final_df[date_col].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg4oduVkv6as"
   },
   "outputs": [],
   "source": [
    "destination_project_id = \"\" #@param {type:\"string\"}\n",
    "destination_dataset = \"RBA_demo\" #@param {type:\"string\"}\n",
    "destination_table = \"cleaned_data\" #@param {type:\"string\"}\n",
    "dataset_table = destination_dataset+\".\"+destination_table\n",
    "\n",
    "final_df.to_gbq(dataset_table,\n",
    "                 destination_project_id,\n",
    "                 chunksize=None,\n",
    "                 if_exists='replace'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-F36U_NmYEc"
   },
   "source": [
    "### 9.2) Prepare the data for optimization tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifTaMFKV2UQt"
   },
   "source": [
    "The budget optimization tool requires the model features in their original un-transformed state as an input. The following function pulls the names of the required columns and collects the data from the relevant dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_IrGN6Amemp"
   },
   "outputs": [],
   "source": [
    "def parse_final_column_features(columns):\n",
    "  final = []\n",
    "  for col in columns:\n",
    "    if any(char.isdigit() for char in col):\n",
    "      splitList = col[::-1].split('l', 1)\n",
    "      parsed = (splitList[1] if len(splitList) > 1 else splitList[0])\n",
    "      final.append(parsed[::-1])\n",
    "    else:\n",
    "      final.append(col)\n",
    "  return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cb6CRk28b5FT"
   },
   "outputs": [],
   "source": [
    "df[df.columns[df.columns.isin(parse_final_column_features(selected_featureset_df.columns))]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUev74EqCELN"
   },
   "outputs": [],
   "source": [
    "optimizer_df = df[df.columns[df.columns.isin(parse_final_column_features(\n",
    "    selected_featureset_df.columns))]]\n",
    "optimizer_df.reset_index(inplace=True, drop = True)\n",
    "#optimizer_df[date_col] = optimizer_df[date_col].astype('string')\n",
    "target_variable.reset_index(inplace=True, drop=True)\n",
    "optimizer_df[kpi_col] = target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnH1inxh0bjR"
   },
   "outputs": [],
   "source": [
    "destination_project_id = \"\" #@param {type:\"string\"}\n",
    "destination_dataset = \"RBA_demo\" #@param {type:\"string\"}\n",
    "destination_table = \"optimizer_data\" #@param {type:\"string\"}\n",
    "dataset_table = destination_dataset+\".\"+destination_table\n",
    "\n",
    "optimizer_df.to_gbq(dataset_table,\n",
    "                 destination_project_id,\n",
    "                 chunksize=None,\n",
    "                 if_exists='replace'\n",
    "                 )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
